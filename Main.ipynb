{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d9539f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for correct pytorch version\n",
    "# import pkg_resources\n",
    "# pkg_resources.require(\"torch==1.11.0\")\n",
    "# pkg_resources.require(\"torchmetrics==0.11.0\")\n",
    "\n",
    "# main net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "# ====================================== #\n",
    "\n",
    "# for dataset\n",
    "from os import listdir, sep\n",
    "\n",
    "from torchvision import transforms\n",
    "# from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# loading raw image\n",
    "from skimage import io\n",
    "\n",
    "# demosaicing raw image\n",
    "from colour_demosaicing import demosaicing_CFA_Bayer_bilinear\n",
    "# ====================================== #\n",
    "\n",
    "# for loss and optimizer\n",
    "import torch.optim as optim\n",
    "from kornia.color import rgb_to_lab\n",
    "from torchmetrics import MultiScaleStructuralSimilarityIndexMeasure as MSSSIM\n",
    "# ====================================== #\n",
    "\n",
    "# quality of life\n",
    "from tqdm import tqdm\n",
    "# ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9bb352-1d41-4b38-bd22-0eb5e4297e8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil, os\n",
    "process = psutil.Process(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327b363a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepispLL(nn.Module):\n",
    "    def __init__(self, kernel=(3,3), stride=1, padding=1):\n",
    "        super(DeepispLL, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(61, 61, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(3, 3, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        rh = self.conv1(x[:,:61,:,:])\n",
    "        rh = self.relu(rh)\n",
    "\n",
    "        lh = self.conv2(x[:,61:,:,:])\n",
    "        lh = self.tanh(lh)\n",
    "\n",
    "        # need to so some sum\n",
    "        # lh += x\n",
    "\n",
    "        return torch.cat((rh, lh), 1)\n",
    "\n",
    "\n",
    "class DeepispHL(nn.Module):\n",
    "    def __init__(self, kernel=(3,3), stride=2, padding=1):\n",
    "        super(DeepispHL, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(64, 64, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, w, h = tuple(x.shape)\n",
    "        return nn.AvgPool2d(kernel_size=(h,w))(x).reshape((b, c))\n",
    "\n",
    "\n",
    "def triu(rgb):\n",
    "    res = torch.tensor(np.empty(10), dtype=torch.float)\n",
    "    r, g, b = rgb[0, 0], rgb[0, 1], rgb[0, 2]\n",
    "    res[0] = r*r\n",
    "    res[1] = r*g\n",
    "    res[2] = r*b\n",
    "    res[3] = r\n",
    "    res[4] = g*g\n",
    "    res[5] = g*b\n",
    "    res[6] = g\n",
    "    res[7] = b*b\n",
    "    res[8] = b\n",
    "    res[9] = 1\n",
    "\n",
    "    return res.reshape((1, 10))\n",
    "\n",
    "\n",
    "def Tform(I, W):\n",
    "    b, c, h, w = I.shape\n",
    "    res = torch.tensor(np.zeros(I.shape))\n",
    "    W = W.reshape((3, 10))\n",
    "    r = torch.tensor(np.zeros(10), dtype=torch.float)\n",
    "    # print('in T before cycle ', process.memory_info().rss / (2 ** 20))  # in mb \n",
    "    for x in range(h):\n",
    "        # print(f'in T inside cycle {x} ', process.memory_info().rss / (2 ** 20))  # in mb \n",
    "        for y in range(w):\n",
    "            # triu(I[:, :, x, y], r)\n",
    "            res[:, :, x, y] = torch.tensordot(W, triu(I[:, :, x, y]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8755bf68-b63a-4267-8c57-b4893dd4a362",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepISP(nn.Module):\n",
    "    def __init__(self, n_ll, n_hl, stride=1, padding=1):\n",
    "        super(DeepISP, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.lowlevel = nn.Sequential()\n",
    "        self.highlevel = nn.Sequential()\n",
    "\n",
    "        self.lowlevel.append(nn.Conv2d(3, 64, kernel_size=(3,3), stride=self.stride, padding=self.padding))\n",
    "        self.highlevel.append(nn.Conv2d(61, 64, kernel_size=(3,3), stride=self.stride, padding=self.padding))\n",
    "\n",
    "        for i in range(n_ll):\n",
    "            self.lowlevel.append(DeepispLL(stride=self.stride, padding=self.padding))\n",
    "\n",
    "        for i in range(n_hl):\n",
    "            self.highlevel.append(DeepispHL(stride=self.stride, padding=self.padding))\n",
    "\n",
    "        # append global pooling on high level to get 1x1x64 shape\n",
    "        # current shape = (N/4^n_hl)*(M/4^n_hl)*64\n",
    "        # self.highlevel.append(nn.MaxPool2(...))\n",
    "        self.highlevel.append(GlobalPool2d())\n",
    "\n",
    "        self.highlevel.append(nn.Linear(64, 30))\n",
    "        \n",
    "        # do some T(W, L)\n",
    "        self.T = Tform\n",
    "    \n",
    "    def forward(self, x):\n",
    "        I = self.lowlevel(x)\n",
    "        W = self.highlevel(I[:,:61,:,:])\n",
    "        x = self.T(I[:,61:,:,:], W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb61e504",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class S7Dataset(Dataset):\n",
    "    def __init__(self, directory, mode, target, factor, crop_size):\n",
    "        self.directory = directory\n",
    "\n",
    "        self.raw_transform = demosaicing_CFA_Bayer_bilinear\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "        self.dng = '.dng'\n",
    "        self.jpg = '.jpg'\n",
    "\n",
    "        self.l = len(listdir(self.directory))\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.len = 0, int(self.l * factor)\n",
    "        if mode == 'test':\n",
    "            self.len = int(self.l * factor), self.l\n",
    "\n",
    "        if target == 'm':\n",
    "            self.target = 'medium_exposure'\n",
    "        elif target == 's':\n",
    "            self.target = 'short_exposure'\n",
    "            self.jpg = '1.jpg'\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.len[1] - self.len[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        l = listdir(self.directory)\n",
    "\n",
    "        i_img = io.imread(sep.join([self.directory, l[idx + self.len[0]], f'{self.target}{self.dng}']))\n",
    "        o_img = io.imread(sep.join([self.directory, l[idx + self.len[0]], f'{self.target}{self.jpg}']))\n",
    "\n",
    "        i_img = self.raw_transform(i_img) / 1024\n",
    "        \n",
    "        old_shape = i_img.shape\n",
    "        new_shape = old_shape[2], self.crop_size, self.crop_size\n",
    "        \n",
    "        x = np.random.randint(0, old_shape[0] - self.crop_size)\n",
    "        y = np.random.randint(0, old_shape[1] - self.crop_size)\n",
    "        \n",
    "        slice_x = slice(x, x + self.crop_size)\n",
    "        slice_y = slice(y, y + self.crop_size)\n",
    "        \n",
    "        i_img = torch.tensor(i_img[slice_x, slice_y, :].copy())\n",
    "        if (old_shape[0], old_shape[1]) != (o_img.shape[0], o_img.shape[1]):            \n",
    "#             print('Bad shape detected', old_shape, o_img.shape)\n",
    "            slice_x, slice_y = slice_y, slice_x\n",
    "        o_img = torch.tensor(o_img[slice_x, slice_y, :].copy())\n",
    "                \n",
    "        i_img = i_img.reshape(new_shape)\n",
    "        o_img = o_img.reshape(new_shape)\n",
    "        \n",
    "        # maybe do data normalization\n",
    "        # img = norm(img)\n",
    "\n",
    "        return i_img.float(), o_img.float()\n",
    "\n",
    "\n",
    "def get_data(data_path, batch_size, target='m', factor=0.7, crop_size=256):\n",
    "    train_data = S7Dataset(\n",
    "        directory=data_path,\n",
    "        mode='train',\n",
    "        target=target,\n",
    "        factor=factor,\n",
    "        crop_size=crop_size\n",
    "    )\n",
    "\n",
    "    test_data = S7Dataset(\n",
    "        directory=data_path,\n",
    "        mode='test',\n",
    "        target=target,\n",
    "        factor=factor,\n",
    "        crop_size=crop_size\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bd69b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class deepISPloss():\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.MSSSIM = MSSSIM()\n",
    "    \n",
    "    def __call__(self, x, target):\n",
    "        lab_x = rgb_to_lab(x).float()\n",
    "        lab_tar = rgb_to_lab(target).float()\n",
    "        b, c, h, w = lab_x.shape\n",
    "\n",
    "        res = (1 - self.alpha) * torch.mean(torch.abs(lab_x - lab_tar))\n",
    "        # take only first channel to MS-SSIM\n",
    "        res +=     self.alpha  * (self.MSSSIM(lab_x[:, :1, :, :], lab_tar[:, :1, :, :]))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b77b17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch number 77\n",
      "test  batch number 33\n",
      "data  835.35546875\n"
     ]
    }
   ],
   "source": [
    "# data_path = '/home/jupyter/mnt/datasets/S7Dataset/S7-ISP-Dataset'\n",
    "data_path = '/home/tima/projects/isp/dataset/S7-ISP-Dataset'\n",
    "train, test = get_data(data_path, batch_size=1, crop_size=256)\n",
    "\n",
    "print(f'train batch number {len(train)}')\n",
    "print(f'test  batch number {len(test)}')\n",
    "\n",
    "print('data ', process.memory_info().rss / (2 ** 20))  # in mb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2357173-283d-4107-91d5-a97fee9090d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# questions:\n",
    "# - sum in low level\n",
    "\n",
    "e = 10\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "make_checkpoints = True\n",
    "# checkpoint_path = '/home/jupyter/work/resources/deepISP-implementation/checkp'\n",
    "checkpoint_path = '/home/tima/projects/isp/deepisp/CP'\n",
    "\n",
    "epochs = [i for i in range(e)]\n",
    "\n",
    "# we can create any number of low level layers\n",
    "# but we can create limited number of high level layers\n",
    "# its because we do pool(2, 2) in every hl layer\n",
    "# so we can create maximum hlc = log2(img_size)\n",
    "# assuming image is a squire matrix with height = width = img_size\n",
    "llc, hlc = 5, 5\n",
    "mtype = str(llc) + str(hlc)\n",
    "model = DeepISP(llc, hlc).float()\n",
    "criterion = deepISPloss()\n",
    "optimizer = optim.SGD(DeepISP.parameters(model), lr, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead0e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainig...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, training: 100%|███████████████████████████████████████████| 77/77 [24:13<00:00, 18.87s/it]\n",
      "Epoch: 0, testing : 100%|██████████████████████████| 33/33 [10:17<00:00, 18.72s/it, str=mem: 1206.03mb,loss: 1352.9022216796875]\n",
      "Epoch: 1, training: 100%|███████████████████████████████████████████| 77/77 [23:58<00:00, 18.68s/it]\n",
      "Epoch: 1, testing : 100%|████████████████████████████| 33/33 [10:16<00:00, 18.67s/it, str=mem: 991.71mb,loss: 1327.275146484375]\n",
      "Epoch: 2, training: 100%|███████████████████████████████████████████| 77/77 [23:55<00:00, 18.64s/it]\n",
      "Epoch: 2, testing :  67%|██████████████████         | 22/33 [06:50<03:25, 18.68s/it, str=mem: 595.25mb,loss: 1547.4312744140625]"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "\n",
    "print('Starting trainig...')\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "for epoch in epochs:\n",
    "    train_iter = tqdm(train, ncols=100, desc='Epoch: {}, training'.format(epoch))\n",
    "    for (x, target) in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x.float())\n",
    "        loss = criterion(y, target)\n",
    "        loss.requires_grad_()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_iter.close()\n",
    "    \n",
    "    test_iter = tqdm(test, ncols=128, desc='Epoch: {}, testing '.format(epoch))\n",
    "    for idx, (x, target) in enumerate(test_iter):\n",
    "        y = model(x)\n",
    "        loss = criterion(y, target)\n",
    "        test_loss += loss\n",
    "        test_iter.set_postfix(str=f'mem: {round(process.memory_info().rss / (2 ** 20), 2)}mb,loss: {test_loss / (idx + 1)}')\n",
    "    test_loss /= len(test_iter)\n",
    "    test_iter.close()\n",
    "    \n",
    "    if make_checkpoints:\n",
    "        torch.save({\n",
    "                        'epoch': 0,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': test_loss,\n",
    "                    }, checkpoint_path + '/model{}_e{}_loss{}'.format(mtype, epoch, test_loss))\n",
    "\n",
    "print('Training done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
